{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ ðŸ§® ðŸ’¡ Text to numbers to insights\n",
    "\n",
    "Coming up in this talk:\n",
    "\n",
    "- A high level intro on neural NLP\n",
    "- Some practical clicking around this notebook to see what it means in practice\n",
    "\n",
    "Not coming up in this talk:\n",
    "\n",
    "- Generative LLMs\n",
    "- ChatGPT\n",
    "- AI Hype\n",
    "\n",
    "We are going to move quickly, do get back to this notebook on your own time or hmu on email or LinkedIn if you want to go deeper âœ¨\n",
    "\n",
    "#### About me\n",
    "\n",
    "- ~8 years in Academia, PhD in NLP 2019\n",
    "- Worked in a couple ML startups\n",
    "- Team lead MI@Fremtind\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ Language is hard\n",
    "\n",
    "> _*yo no soy marinero, soy capitan*_\n",
    "\n",
    "There are many languages\n",
    "\n",
    "> _*that jaguar is sick*_\n",
    "\n",
    "Language is contextual\n",
    "\n",
    "> _*a boujee elon stan*_\n",
    "\n",
    "Language is constantly evolving\n",
    "\n",
    "#### How do we represent words?\n",
    "\n",
    "- Tabular data is collected facts\n",
    "- Pictures are RGB matrices\n",
    "- Words (and meaningful sequences of words) have been problematic\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ðŸŽ“ Distributional semantics\n",
    "\n",
    "> _You shall know a word by the company it keeps_ (Firth, 1957)\n",
    "\n",
    "```\n",
    "My son loves to eat [bananas].\n",
    "[Cookies] are sweet.\n",
    "```\n",
    "\n",
    "- The meaning of a word is a function of the meaning of the words it co-occurs with ðŸ¤¯\n",
    "\n",
    "But how do we express that function? What is its output?\n",
    "â†’ We need some sort of universal function approximator\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”® Neural representations\n",
    "\n",
    "**Word2Vec**, The first successfull neural language representation, extremely high level:\n",
    "\n",
    "- Get a lot of data\n",
    "- Split all the words\n",
    "- Train a feed-forward network to predict a vector of a word given the vectors of the surrounding words\n",
    "- Update the word vectors by backpropagating the error\n",
    "\n",
    "â†’ You end up with a dense representation for each word. This representation has semantic properties!\n",
    "\n",
    "```\n",
    "distance(vectors[\"banana\"], vectors[\"cake\"]) < distance(vectors[\"banana\"], vectors[\"lego\"])`\n",
    "\n",
    "vectors.most_similar(vectors[\"apple\"] - vectors[\"fruit\"] + vectors[\"potato\"])\n",
    "=> \"vegetable\"\n",
    "```\n",
    "\n",
    "### Nowadays\n",
    "\n",
    "- Longer text representations\n",
    "- Contextual embeddings\n",
    "- Transformer architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "PREPROCESS = spacy.load(\"en_core_web_sm\") \n",
    "TRANSFORMER = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return \" \".join(soup.get_text().split())\n",
    "\n",
    "\n",
    "def get_data_df(data, name=pd.NA):\n",
    "    data = PREPROCESS(data)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"text\"] = [sentence.text for sentence in data.sents if len(sentence.text) > 80]\n",
    "    df[\"embedding\"] = list(TRANSFORMER.encode(list(df[\"text\"])))\n",
    "    df[\"name\"] = [name for _ in range(len(df))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def reduce_dimensions(df):\n",
    "    reduced = UMAP().fit_transform(list(df[\"embedding\"]))\n",
    "    df[\"x\"], df[\"y\"] = reduced[:, 0], reduced[:, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”µ ðŸ”´ ðŸŸ¢ Representing wikipedia articles with a Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blading = get_data_df(get_text(\"https://en.wikipedia.org/wiki/Inline_skating\"), \"blading\")\n",
    "muppets = get_data_df(get_text(\"https://en.wikipedia.org/wiki/The_Muppets\"), \"muppets\")\n",
    "skateboarding = get_data_df(get_text(\"https://en.wikipedia.org/wiki/Skateboarding\"), \"skateboarding\")\n",
    "smurfs = get_data_df(get_text(\"https://en.wikipedia.org/wiki/The_Smurfs\"), \"smurfs\")\n",
    "\n",
    "df = pd.concat([blading, muppets, skateboarding, smurfs])\n",
    "df = reduce_dimensions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alt.Chart(df.drop_duplicates(subset=[\"text\"])).mark_circle(size=100).encode(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    tooltip=[\"text\", \"name\"],\n",
    "    color=\"name\"\n",
    ").properties(width=800, height=500).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”® Semantic search on the cheap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "query = \"what are other common names for inline skating?\"\n",
    "embedded_query = TRANSFORMER.encode([query])[0]\n",
    "df[\"distance\"] = [cosine(embedded_query, embedding) for embedding in df[\"embedding\"]]\n",
    "\n",
    "list(df.sort_values(by=\"distance\")[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”® Classification with no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = [\n",
    "    \"sports\" if x in (\"blading\", \"skateboarding\") else \"tv-shows\" \n",
    "    for x in df[\"name\"]\n",
    "]\n",
    "\n",
    "# Try it out with other articles!\n",
    "wiki = get_text(\"https://en.wikipedia.org/wiki/Seinfeld\")\n",
    "embedded_wiki = TRANSFORMER.encode([wiki])[0]\n",
    "df[\"distance\"] = [cosine(embedded_wiki, embedding) for embedding in df[\"embedding\"]]\n",
    "\n",
    "df.sort_values(by=\"distance\")[\"label\"].head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "51b3b1ebd12dad03c3dac59bdb3fde3641884310b29eda4483a646cec0ef6183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
