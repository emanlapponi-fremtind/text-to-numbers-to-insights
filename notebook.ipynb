{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch spacy umap-learn pandas altair sentence-transformers scipy beautifulsoup4\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download nb_core_news_sm\n",
    "!python -c \"from sentence_transformers import SentenceTransformer; SentenceTransformer('distiluse-base-multilingual-cased-v2');\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ ðŸ§® ðŸ’¡ **Text to numbers to insights**\n",
    "\n",
    "Coming up in this talk:\n",
    "\n",
    "- A high level intro on neural NLP: How do we get from text to numbers?\n",
    "- Some practical clicking around this notebook to see what it means in practice\n",
    "\n",
    "**Not** coming up in this talk:\n",
    "\n",
    "- Generative LLMs\n",
    "- ChatGPT\n",
    "- AI Hype\n",
    "\n",
    "We are going to move quickly, do get back to this notebook on your own time or hmu on email or LinkedIn if you want to go deeper âœ¨\n",
    "\n",
    "#### ðŸ‘¨ðŸ» **About me**\n",
    "\n",
    "- ~8 years in Academia, PhD in NLP 2019\n",
    "- Worked in a couple ML startups\n",
    "- Team lead MI@Fremtind\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ **Language is hard**\n",
    "\n",
    "> _*yo no soy marinero, soy capitan*_\n",
    "\n",
    "There are many languages\n",
    "\n",
    "> _*that jaguar is sick*_\n",
    "\n",
    "Language is contextual\n",
    "\n",
    "> _*a boujee elon stan*_\n",
    "\n",
    "Language is constantly evolving\n",
    "\n",
    "#### **How do we represent language?**\n",
    "\n",
    "- Tabular data is collected facts\n",
    "- Pictures are RGB matrices\n",
    "- Words (and meaningful sequences of words) have been problematic\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ðŸŽ“ **Distributional semantics**\n",
    "\n",
    "> _You shall know a word by the company it keeps_ (Firth, 1957)\n",
    "\n",
    "```\n",
    "My son loves to eat [bananas].\n",
    "[Cookies] are sweet.\n",
    "[Apples] grow on trees.\n",
    "I love dunking [biscuits] in milk.\n",
    "```\n",
    "\n",
    "- The meaning of a word is a function of the meaning of the words it co-occurs with ðŸ¤¯\n",
    "\n",
    "But how do we express that function? What is its output?\n",
    "â†’ We need some sort of universal function approximator\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”® **Neural representations**\n",
    "\n",
    "**Word2Vec**, The first successfull neural language representation, extremely high level:\n",
    "\n",
    "- Get a lot of data\n",
    "- Split all the words\n",
    "- Train a feed-forward network to predict a vector of a word given the vectors of the surrounding words\n",
    "- Update the word vectors by backpropagating the error\n",
    "\n",
    "â†’ You end up with a dense representation for each word. This representation has semantic properties!\n",
    "\n",
    "```python\n",
    "distance(vectors[\"banana\"], vectors[\"cake\"]) < distance(vectors[\"banana\"], vectors[\"lego\"])\n",
    "\n",
    "vectors.most_similar(vectors[\"apple\"] - vectors[\"fruit\"] + vectors[\"potato\"])\n",
    "=> \"vegetable\"\n",
    "```\n",
    "\n",
    "#### **Nowadays**\n",
    "\n",
    "- Contextual embeddings\n",
    "- Longer text representations, eg sentences, documents\n",
    "- Transformer architectures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "PREPROCESS = spacy.load(\"en_core_web_sm\") \n",
    "TRANSFORMER = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "DEVICE = \"mps\" # \"cpu\" or \"cuda\"\n",
    "\n",
    "\n",
    "def get_text(url):\n",
    "    page = urlopen(url)\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return \" \".join(soup.get_text().split())\n",
    "\n",
    "\n",
    "def get_data_df(data, name=pd.NA, limit=80):\n",
    "    data = PREPROCESS(data)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"text\"] = [sentence.text for sentence in data.sents if len(sentence.text) > limit]\n",
    "    df[\"embedding\"] = list(TRANSFORMER.encode(list(df[\"text\"]), device=torch.device(DEVICE)))\n",
    "    df[\"name\"] = [name for _ in range(len(df))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def reduce_dimensions(df):\n",
    "    reduced = UMAP(random_state=42).fit_transform(list(df[\"embedding\"]))\n",
    "    df[\"x\"], df[\"y\"] = reduced[:, 0], reduced[:, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    (\"banana\", \"banana\", 0),\n",
    "    (\"apple\", \"apple\", 0), \n",
    "    (\"pineapple\", \"pineapple\", 0), \n",
    "    (\"orange\", \"orange\", 0),\n",
    "    (\"pear\", \"pear\", 0),\n",
    "    (\"grape\", \"grape\", 0),\n",
    "    (\"strawberry\", \"strawberry\", 0),\n",
    "    (\"work\", \"work\", 0),\n",
    "    (\"school\", \"school\", 0),\n",
    "    (\"university\", \"university\", 0),\n",
    "    (\"college\", \"college\", 0),\n",
    "    (\"student\", \"student\", 0),\n",
    "]\n",
    "words = reduce_dimensions(pd.concat([get_data_df(*x) for x in words]))\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(words).mark_circle(size=100).encode(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    tooltip=[\"text\", \"name\"],\n",
    "    color=\"name\"\n",
    ").properties(width=500, height=300).interactive()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ”µ ðŸ”´ ðŸŸ¢ **Representing wikipedia articles with a Transformer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blading = get_data_df(get_text(\"https://en.wikipedia.org/wiki/Inline_skating\"), \"blading\")\n",
    "muppets = get_data_df(get_text(\"https://en.wikipedia.org/wiki/The_Muppets\"), \"muppets\")\n",
    "skateboarding = get_data_df(get_text(\"https://en.wikipedia.org/wiki/Skateboarding\"), \"skateboarding\")\n",
    "smurfs = get_data_df(get_text(\"https://en.wikipedia.org/wiki/The_Smurfs\"), \"smurfs\")\n",
    "tennis = get_data_df(get_text(\"https://en.wikipedia.org/wiki/Tennis\"), \"tennis\")\n",
    "seinfeld = get_data_df(get_text(\"https://en.wikipedia.org/wiki/Seinfeld\"), \"seinfeld\")\n",
    "\n",
    "df = pd.concat([blading, muppets, skateboarding, smurfs, tennis, seinfeld])\n",
    "df = reduce_dimensions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alt.Chart(df.drop_duplicates(subset=[\"text\"])).mark_circle(size=100).encode(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    tooltip=[\"text\", \"name\"],\n",
    "    color=\"name\"\n",
    ").properties(width=1500, height=800).interactive()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ”® **Semantic search on the cheap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from pprint import pprint\n",
    "\n",
    "query = \"How does the smurf language work?\"\n",
    "embedded_query = TRANSFORMER.encode([query])[0]\n",
    "df[\"distance\"] = [cosine(embedded_query, embedding) for embedding in df[\"embedding\"]]\n",
    "\n",
    "pprint(df.sort_values(by=\"distance\")[\"text\"].head(3).tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ”® **Classification with no training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = [\n",
    "    \"sports\" if x in (\"blading\", \"skateboarding\", \"tennis\") else \"tv-shows\" \n",
    "    for x in df[\"name\"]\n",
    "]\n",
    "\n",
    "wiki = get_text(\"https://en.wikipedia.org/wiki/Futurama\")\n",
    "embedded_wiki = TRANSFORMER.encode([wiki])[0]\n",
    "df[\"distance\"] = [cosine(embedded_wiki, embedding) for embedding in df[\"embedding\"]]\n",
    "\n",
    "df.sort_values(by=\"distance\")[\"label\"].head(10).value_counts().idxmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "51b3b1ebd12dad03c3dac59bdb3fde3641884310b29eda4483a646cec0ef6183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
